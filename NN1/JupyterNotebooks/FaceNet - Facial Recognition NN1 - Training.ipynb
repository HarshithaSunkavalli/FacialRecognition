{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from matplotlib.pyplot import imshow\n",
    "from preprocessing import *\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders_for_training(n_H0, n_W0, n_C0):\n",
    "    # n_H0 -- scalar, height of an input image\n",
    "    # n_W0 -- scalar, width of an input image\n",
    "    # n_C0 -- scalar, number of channels of the input\n",
    "    # Returns\n",
    "    # X,Y,Z -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    X = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Z = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    return X, Y, Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(n_C0):\n",
    "    tf.set_random_seed(1)\n",
    "    conv1 = tf.get_variable(\"conv1\", [7,7,n_C0,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2a= tf.get_variable(\"conv2a\", [1,1,64,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv2 = tf.get_variable(\"conv2\", [3,3,64,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3a= tf.get_variable(\"conv3a\", [1,1,192,192], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv3 = tf.get_variable(\"conv3\", [3,3,192,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4a= tf.get_variable(\"conv4a\", [1,1,384,384], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv4 = tf.get_variable(\"conv4\", [3,3,384,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5a= tf.get_variable(\"conv5a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv5 = tf.get_variable(\"conv5\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6a= tf.get_variable(\"conv6a\", [1,1,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    conv6 = tf.get_variable(\"conv6\", [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc1 = tf.get_variable(\"fc1\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc2   = tf.get_variable(\"fc2\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    #fc7128= tf.get_variable(\"fc7128\", [7,7,,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    parameters = {\"conv1\": conv1,\n",
    "                  \"conv2a\": conv2a,\n",
    "                  \"conv2\": conv2,\n",
    "                  \"conv3a\": conv3a,\n",
    "                  \"conv3\": conv3,\n",
    "                  \"conv4a\": conv4a,\n",
    "                  \"conv4\": conv4,\n",
    "                  \"conv5a\": conv5a,\n",
    "                  \"conv5\": conv5,\n",
    "                  \"conv6a\": conv6a,\n",
    "                  \"conv6\": conv6,\n",
    "                  #\"fc1\": fc1,\n",
    "                  #\"fc2\": fc2,\n",
    "                  #\"fc7128\": fc7128,\n",
    "                  }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(parameters,x):\n",
    "    \n",
    "    conv1 = parameters['conv1']\n",
    "    conv2a = parameters['conv2a']\n",
    "    conv2 = parameters['conv2']\n",
    "    conv3a = parameters['conv3a']\n",
    "    conv3 = parameters['conv3']\n",
    "    conv4a = parameters['conv4a']\n",
    "    conv4 = parameters['conv4']\n",
    "    conv5a = parameters['conv5a']\n",
    "    conv5 = parameters['conv5']\n",
    "    conv6a = parameters['conv6a']\n",
    "    conv6 = parameters['conv6']\n",
    "    \n",
    "    #Conv 1\n",
    "    Z1 = tf.nn.conv2d(x,conv1, strides = [1,2,2,1], padding = 'VALID')\n",
    "    with tf.variable_scope(\"b1\") as scope:\n",
    "        B1 = tf.layers.batch_normalization(Z1,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A1 = tf.nn.relu(B1)\n",
    "        P1 = tf.nn.max_pool(A1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        N1 = tf.nn.lrn(P1)\n",
    "    \n",
    "    #Conv 2A\n",
    "    Z2a = tf.nn.conv2d(N1,conv2a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A2a = tf.nn.relu(Z2a)\n",
    "   \n",
    "    #Conv 2\n",
    "    Z2 = tf.nn.conv2d(A2a,conv2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b2\") as scope:\n",
    "        B2 = tf.layers.batch_normalization(Z2,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A2 = tf.nn.relu(B2)\n",
    "        P2 = tf.nn.max_pool(A2, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        N2 = tf.nn.lrn(P2)\n",
    "     \n",
    "    #Conv 3A\n",
    "    Z3a = tf.nn.conv2d(N2,conv3a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A3a = tf.nn.relu(Z3a)  \n",
    "    \n",
    "    #Conv 3\n",
    "    Z3 = tf.nn.conv2d(A3a,conv3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b3\") as scope:\n",
    "        B3 = tf.layers.batch_normalization(Z3,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A3 = tf.nn.relu(B3)\n",
    "        P3 = tf.nn.max_pool(A3, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        N3 = tf.nn.lrn(P3)\n",
    "    \n",
    "    #Conv 4a\n",
    "    Z4a = tf.nn.conv2d(N3,conv4a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A4a = tf.nn.relu(Z4a)\n",
    "    \n",
    "    #Conv 4\n",
    "    Z4 = tf.nn.conv2d(A4a,conv4, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b4\") as scope:\n",
    "        B4 = tf.layers.batch_normalization(Z4,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A4 = tf.nn.relu(B4)\n",
    "        N4 = tf.nn.lrn(A4)\n",
    "    \n",
    "    #Conv 5a\n",
    "    Z5a = tf.nn.conv2d(N4,conv5a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A5a = tf.nn.relu(Z5a)\n",
    "    \n",
    "    #Conv 5\n",
    "    Z5 = tf.nn.conv2d(A5a,conv5, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b5\") as scope:\n",
    "        B5 = tf.layers.batch_normalization(Z5,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A5 = tf.nn.relu(B5)\n",
    "        N5 = tf.nn.lrn(A5)\n",
    "    \n",
    "    #Conv 6a\n",
    "    Z6a = tf.nn.conv2d(N5,conv6a, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A6a = tf.nn.relu(Z6a)\n",
    "    \n",
    "    #Conv 6\n",
    "    Z6 = tf.nn.conv2d(A6a,conv6, strides = [1,1,1,1], padding = 'SAME')\n",
    "    with tf.variable_scope(\"b6\") as scope:\n",
    "        B6 = tf.layers.batch_normalization(Z6,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n",
    "        A6 = tf.nn.relu(B6)\n",
    "        P6 = tf.nn.max_pool(A6, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #Flattening\n",
    "    P6F = tf.contrib.layers.flatten(P6)\n",
    "    \n",
    "    #FC 1\n",
    "    with tf.variable_scope(\"fc1\") as scope:\n",
    "        Z_FC1 = tf.contrib.layers.fully_connected(P6F,32*256,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC1 = tf.nn.relu(Z_FC1)\n",
    "    #Maxout\n",
    "    #M_FC1 = tf.contrib.layers.maxout(A_FC1,32*128)\n",
    "    \n",
    "    #FC_2\n",
    "    with tf.variable_scope(\"fc2\") as scope:\n",
    "        Z_FC2 = tf.contrib.layers.fully_connected(A_FC1,32*256,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC2 = tf.nn.relu(Z_FC2)\n",
    "\n",
    "    #Maxout\n",
    "    #M_FC2 = tf.contrib.layers.maxout(A_FC2,32*128)\n",
    "    \n",
    "    #FC_7128\n",
    "    with tf.variable_scope(\"fc3\") as scope:\n",
    "        Z_FC7 = tf.contrib.layers.fully_connected(A_FC2,128,activation_fn=None,reuse = tf.AUTO_REUSE,scope = tf.get_variable_scope())\n",
    "        A_FC7 = tf.nn.relu(Z_FC7)\n",
    "\n",
    "    #l2 Normalization\n",
    "    embeddings = tf.nn.l2_normalize(A_FC7)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss_debug(y_pred, alpha = 0.5):\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "   \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "    \n",
    "    pos_dist2 = tf.Print(pos_dist, [pos_dist], \"pos_dist \")\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "    \n",
    "    neg_dist2 = tf.Print(neg_dist, [neg_dist], \"neg_dist \")\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist2, neg_dist2) , alpha)\n",
    "    basic_loss2 = tf.Print(basic_loss, [basic_loss], \"basic loss: \")\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss2,0.0))\n",
    "    \n",
    "    loss2 = tf.Print(loss, [loss], \"loss \")\n",
    "    \n",
    "    return loss2\n",
    "\n",
    "def triplet_loss(y_pred, alpha = 0.5):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis = -1)\n",
    "   \n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis = -1)\n",
    "    \n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist) , alpha)\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSampleData():\n",
    "    fname = \"./SampleDataset/1a.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    anchor = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1b.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    positive = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    \n",
    "    fname = \"./SampleDataset/1c.jpeg\"\n",
    "    image = cv2.imread(fname)\n",
    "    \n",
    "    negative = np.expand_dims(cv2.resize(image, (220,220),interpolation = cv2.INTER_AREA),axis = 0)\n",
    "    #print(negative.shape)\n",
    "    \n",
    "    return anchor,positive,negative\n",
    "    #return 0,0,0\n",
    "\n",
    "#anchor,positive,negative = loadSampleData()\n",
    "def getLoaderInstance(batchSize = 64, batches = 15):\n",
    "    loaderInstance = DataReader(batchSize,batches)\n",
    "    return loaderInstance\n",
    "\n",
    "def loadDataBatch(loaderInstance):\n",
    "    tempList = loaderInstance.getData()\n",
    "    anchor, positive, negative = (np.array(tempList[0]),np.array(tempList[1]),np.array(tempList[2]))\n",
    "    #print(anchor.shape)\n",
    "    return anchor,positive,negative\n",
    "\n",
    "#anchor,positive,negative = loadDataBatch()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Loader to start Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3113424e2a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataLoaderInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetLoaderInstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-3d425aebd8c3>\u001b[0m in \u001b[0;36mgetLoaderInstance\u001b[1;34m(batchSize, batches)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#anchor,positive,negative = loadSampleData()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetLoaderInstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloaderInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloaderInstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataReader' is not defined"
     ]
    }
   ],
   "source": [
    "batchSize = 64\n",
    "batches = 40\n",
    "dataLoaderInstance = getLoaderInstance(batchSize, batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "(64, 220, 220, 3)\n"
     ]
    }
   ],
   "source": [
    "anchor = np.empty((batchSize,220,220,3))\n",
    "positive = np.empty((batchSize,220,220,3))\n",
    "negative = np.empty((batchSize,220,220,3))\n",
    "for i in range(batches):\n",
    "    anchor,positive,negative = loadDataBatch(dataLoaderInstance)\n",
    "    with open('./cache/inputs'+str(i)+'.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([anchor, positive, negative], f, 0)\n",
    "    if(i%10==0):\n",
    "        print(i)\n",
    "        \n",
    "print(anchor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching the loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('inputs6415.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([anchor, positive, negative], f)\n",
    "# imshow(positive[1])\n",
    "# import cv2\n",
    "# cv2.imwrite(\"image.jpg\", negative[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 220, 220, 3)\n"
     ]
    }
   ],
   "source": [
    "with open('../inputs6415.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    anchor,positive,negative = pickle.load(f)\n",
    "print(anchor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadCache(iter):\n",
    "    with open('./cache/inputs'+str(iter)+'.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "        anchor,positive,negative = pickle.load(f)\n",
    "    return anchor,positive,negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"FaceNet\", reuse=tf.AUTO_REUSE):\n",
    "    x,y,z = create_placeholders_for_training(220,220,3)\n",
    "    params = init_params(3)\n",
    "    preds1 = forward_prop(params,x)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    preds2 = forward_prop(params,y)\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    preds3 = forward_prop(params,z)\n",
    "\n",
    "loss = triplet_loss([preds1,preds2,preds3],0.5)\n",
    "optim = tf.train.AdamOptimizer(0.0001,name = 'optim').minimize(loss)\n",
    "\n",
    "init  = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Saved Model Found. Starting training from scratch.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.\n\t [[Node: FaceNet/b1_2/batch_normalization/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FaceNet/Conv2D_22, FaceNet/b1/batch_normalization/gamma/read, FaceNet/b1/batch_normalization/beta/read, FaceNet/b1/batch_normalization/moving_mean/read, FaceNet/b1/batch_normalization/moving_variance/read)]]\n\nCaused by op 'FaceNet/b1_2/batch_normalization/FusedBatchNorm', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-4cd814d0adb2>\", line 9, in <module>\n    preds3 = forward_prop(params,z)\n  File \"<ipython-input-36-7c68b156b7e3>\", line 18, in forward_prop\n    B1 = tf.layers.batch_normalization(Z1,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 777, in batch_normalization\n    return layer.apply(inputs, training=training)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 809, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 696, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 508, in call\n    outputs = self._fused_batch_norm(inputs, training=training)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 399, in _fused_batch_norm\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 209, in smart_cond\n    return fn2()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 396, in _fused_batch_norm_inference\n    data_format=self._data_format)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 906, in fused_batch_norm\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2569, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.\n\t [[Node: FaceNet/b1_2/batch_normalization/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FaceNet/Conv2D_22, FaceNet/b1/batch_normalization/gamma/read, FaceNet/b1/batch_normalization/beta/read, FaceNet/b1/batch_normalization/moving_mean/read, FaceNet/b1/batch_normalization/moving_variance/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.\n\t [[Node: FaceNet/b1_2/batch_normalization/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FaceNet/Conv2D_22, FaceNet/b1/batch_normalization/gamma/read, FaceNet/b1/batch_normalization/beta/read, FaceNet/b1/batch_normalization/moving_mean/read, FaceNet/b1/batch_normalization/moving_variance/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-f057589f3045>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m#anchor,positive,negative = loadCache(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mcurr_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mavgCost\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mcurr_cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.\n\t [[Node: FaceNet/b1_2/batch_normalization/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FaceNet/Conv2D_22, FaceNet/b1/batch_normalization/gamma/read, FaceNet/b1/batch_normalization/beta/read, FaceNet/b1/batch_normalization/moving_mean/read, FaceNet/b1/batch_normalization/moving_variance/read)]]\n\nCaused by op 'FaceNet/b1_2/batch_normalization/FusedBatchNorm', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-4cd814d0adb2>\", line 9, in <module>\n    preds3 = forward_prop(params,z)\n  File \"<ipython-input-36-7c68b156b7e3>\", line 18, in forward_prop\n    B1 = tf.layers.batch_normalization(Z1,axis=1, epsilon=0.00001,reuse = tf.AUTO_REUSE)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 777, in batch_normalization\n    return layer.apply(inputs, training=training)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 809, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 696, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 508, in call\n    outputs = self._fused_batch_norm(inputs, training=training)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 399, in _fused_batch_norm\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 209, in smart_cond\n    return fn2()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 396, in _fused_batch_norm_inference\n    data_format=self._data_format)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 906, in fused_batch_norm\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2569, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.\n\t [[Node: FaceNet/b1_2/batch_normalization/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FaceNet/Conv2D_22, FaceNet/b1/batch_normalization/gamma/read, FaceNet/b1/batch_normalization/beta/read, FaceNet/b1/batch_normalization/moving_mean/read, FaceNet/b1/batch_normalization/moving_variance/read)]]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 4\n",
    "    #saver.restore(sess, './faceNet3')\n",
    "    train_cache_file = './faceNet31.meta'\n",
    "    if os.path.exists(train_cache_file):\n",
    "        saver.restore(sess, './faceNet31')\n",
    "        print(\"Restoring Model\")\n",
    "    else:\n",
    "        print(\"No Saved Model Found. Starting training from scratch.\")\n",
    "    for epoch in range(epochs):\n",
    "        avgCost = 0\n",
    "        iters = 7\n",
    "        for i in range(iters):\n",
    "            #anchor,positive,negative = loadCache(i)\n",
    "            curr_cost, _  = sess.run([loss, optim],feed_dict = {x:anchor[i*8:i*8+7],y:positive[i*8:i*8+7],z:negative[i*8:i*8+7]})\n",
    "            avgCost+=curr_cost\n",
    "            if(i%2==0):\n",
    "                print(\"i:\"+str(i)+\", loss = \"+str(curr_cost))\n",
    "            #print(embed1)\n",
    "            #print(embed2)\n",
    "        avgCost/=iters\n",
    "        print(\"Avg Loss :\"+str(avgCost))\n",
    "    \n",
    "    saver.save(sess, './faceNet31')\n",
    "    print(\"Model Saved to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDistanceBetweenEmbeddings(embedding1, embedding2):\n",
    "    dist = np.sum(np.square(np.subtract(embedding1,embedding2)))\n",
    "    return dist\n",
    "\n",
    "# anchor = tf.placeholder(tf.float32, shape=(None,220,220,3))\n",
    "# positive = tf.placeholder(tf.float32, shape=(None,220,220,3))\n",
    "# pos_dist2 = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "\n",
    "emb1 = sess.run(preds1,feed_dict = {x:np.expand_dims(anchor[16],axis =0)})\n",
    "emb2 = sess.run(preds1,feed_dict = {x:np.expand_dims(negative[16],axis =0)})\n",
    "\n",
    "#tfdist = sess.run(pos_dist2,feed_dict = {anchor:np.expand_dims(anchor[16],axis =0),positive:np.expand_dims(positive[16],axis =0)})\n",
    "\n",
    "#emb3 = sess.run(preds1,feed_dict = {x:np.expand_dims(anchor[16],axis =0)})\n",
    "emb4 = sess.run(preds1,feed_dict = {x:np.expand_dims(positive[16],axis =0)})\n",
    "\n",
    "print(emb1)\n",
    "print(emb4)\n",
    "dist = getDistanceBetweenEmbeddings(emb1,emb2)\n",
    "print(dist)\n",
    "dist2 = getDistanceBetweenEmbeddings(emb1,emb4)\n",
    "print(dist2)\n",
    "#print(tfdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my-firstmodel1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
